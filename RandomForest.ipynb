{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data analysis\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Independent Variable\n",
    "train_tweet_data = pd.read_csv('train.csv').iloc[:,1]\n",
    "\n",
    "#Dependent Variable\n",
    "train_sentiments = pd.read_csv('train.csv').iloc[:,5]\n",
    "\n",
    "#Data in which we have to predict values\n",
    "test_tweet_data = pd.read_csv('test.csv').iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciton for Cleaning Data\n",
    "    \n",
    "    The given datasets are comprised of very much unstructured tweets which should be preprocessed to make an NLP model. In this project, we tried out the following techniques of preprocessing the raw data. But the preprocessing techniques is not limited.\n",
    "        --Removal of punctuations.\n",
    "        --Removal of commonly used words (stopwords).\n",
    "        --Normalization of words(Stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing Library for Data Cleaning \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#function for Data Cleaning\n",
    "def cleaned_tweets(tweet_data):\n",
    "    \n",
    "    corpus = []\n",
    "    for tweets in tweet_data:\n",
    "        \n",
    "        cleaned_tweets = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweets)\n",
    "        cleaned_tweets = cleaned_tweets.lower().split()\n",
    "        ps = PorterStemmer()\n",
    "        \n",
    "        all_stopwords = stopwords.words('english')\n",
    "        all_stopwords.remove('not') \n",
    "        tweets_words = [ps.stem(word) for word in cleaned_tweets if not word in set(all_stopwords)]\n",
    "        \n",
    "        #joining all cleaned words and making list of it\n",
    "        cleaned_tweets = ' '.join(tweets_words)             \n",
    "        corpus.append(cleaned_tweets)\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Model\n",
    "\n",
    "    Bag-of-words model is a way of extracting features from text for use in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing library for Bag of words Model \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#making X and y matrix: X contains corpus (dependent variable) \n",
    "#y contains sentiments 1 (positive), -1 (Negative) , 0 (Neutral)\n",
    "X = cv.fit_transform(cleaned_tweets(train_tweet_data)).toarray()\n",
    "y = train_sentiments.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "    \n",
    "    Input data is high dimensional data, so reducing the dimensionality by projecting the data to a lower dimensional       subspace which captures the “essence” of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality Reduction\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA( kernel = 'rbf')\n",
    "X = kpca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators =10, criterion = 'entropy',random_state=1)\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Test Data and Preparing Test Data for Predicting sentiments from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Test Data\n",
    "test_X = cleaned_tweets(test_tweet_data.iloc[:,1])\n",
    "\n",
    "#extracting features from test data using Bag of Word Model and converting into numpy array\n",
    "test_X = cv.transform(test_X).toarray()\n",
    "\n",
    "#Applying Dimensionality reduction to Test Data\n",
    "test_X = kpca.transform(test_X)\n",
    "\n",
    "#Predicting sentiments 1 (positive), -1 (Negative) , 0 (Neutral) for test data represented by y_new\n",
    "y_new = classifier.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data into submission format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id  sentiment_class\n",
      "0     1.246628e+18              0.0\n",
      "1     1.245898e+18             -1.0\n",
      "2     1.244717e+18             -1.0\n",
      "3     1.245730e+18              1.0\n",
      "4     1.244636e+18              0.0\n",
      "...            ...              ...\n",
      "1382  1.245219e+18              0.0\n",
      "1383  1.245882e+18             -1.0\n",
      "1384  1.246461e+18              0.0\n",
      "1385  1.246245e+18              0.0\n",
      "1386  1.245178e+18              0.0\n",
      "\n",
      "[1387 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Adding ID and sentiments columnwise and making dataframe of it\n",
    "predicted_sentiments_test_data = np.column_stack((test_tweet_data.iloc[:,0],y_new))\n",
    "predicted_sentiments_test_data= pd.DataFrame(predicted_sentiments_test_data,columns=[\"id\",'sentiment_class'])\n",
    "print(predicted_sentiments_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving predicted sentiments to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiments_test_data.to_csv('Twitter_sentiments_Analysis2.csv',index=False,columns=[\"id\",'sentiment_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8085f59827e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
